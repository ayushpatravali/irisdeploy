{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting pickleshareNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached pickleshare-0.7.5-py2.py3-none-any.whl.metadata (1.5 kB)\n",
      "Using cached pickleshare-0.7.5-py2.py3-none-any.whl (6.9 kB)\n",
      "Installing collected packages: pickleshare\n",
      "Successfully installed pickleshare-0.7.5\n"
     ]
    }
   ],
   "source": [
    "%pip install pickleshare\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "GjYtu0ALPHgl",
    "outputId": "e3790c8d-2d66-4580-a5bd-a56f62f845f1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "d:\\SENSEMI\\yolov5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Cloning into 'yolov5'...\n",
      "d:\\SENSEMI\\.venv\\lib\\site-packages\\IPython\\core\\magics\\osm.py:417: UserWarning: This is now an optional IPython functionality, setting dhist requires you to install the `pickleshare` library.\n",
      "  self.shell.db['dhist'] = compress_dhist(dhist)[-100:]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Setup complete. Using torch 2.5.1+cpu (CPU)\n"
     ]
    }
   ],
   "source": [
    "!git clone https://github.com/ultralytics/yolov5  \n",
    "%cd yolov5\n",
    "%pip install -qr requirements.txt \n",
    "%pip install -q roboflow\n",
    "\n",
    "import torch\n",
    "import os\n",
    "from IPython.display import Image, clear_output \n",
    "\n",
    "print(f\"Setup complete. Using torch {torch.__version__} ({torch.cuda.get_device_properties(0).name if torch.cuda.is_available() else 'CPU'})\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: roboflow in d:\\sensemi\\.venv\\lib\\site-packages (1.1.50)\n",
      "Requirement already satisfied: certifi in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (2024.12.14)\n",
      "Requirement already satisfied: idna==3.7 in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (3.7)\n",
      "Requirement already satisfied: cycler in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (0.12.1)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (1.4.7)\n",
      "Requirement already satisfied: matplotlib in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (3.10.0)\n",
      "Requirement already satisfied: numpy>=1.18.5 in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (2.2.0)\n",
      "Requirement already satisfied: opencv-python-headless==4.10.0.84 in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (4.10.0.84)\n",
      "Requirement already satisfied: Pillow>=7.1.2 in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (11.0.0)\n",
      "Requirement already satisfied: python-dateutil in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (2.9.0.post0)\n",
      "Requirement already satisfied: python-dotenv in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (1.0.1)\n",
      "Requirement already satisfied: requests in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (2.32.3)\n",
      "Requirement already satisfied: six in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (1.17.0)\n",
      "Requirement already satisfied: urllib3>=1.26.6 in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.41.0 in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (4.67.1)\n",
      "Requirement already satisfied: PyYAML>=5.3.1 in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (6.0.2)\n",
      "Requirement already satisfied: requests-toolbelt in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (1.0.0)\n",
      "Requirement already satisfied: filetype in d:\\sensemi\\.venv\\lib\\site-packages (from roboflow) (1.2.0)\n",
      "Requirement already satisfied: colorama in d:\\sensemi\\.venv\\lib\\site-packages (from tqdm>=4.41.0->roboflow) (0.4.6)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in d:\\sensemi\\.venv\\lib\\site-packages (from matplotlib->roboflow) (1.3.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in d:\\sensemi\\.venv\\lib\\site-packages (from matplotlib->roboflow) (4.55.3)\n",
      "Requirement already satisfied: packaging>=20.0 in d:\\sensemi\\.venv\\lib\\site-packages (from matplotlib->roboflow) (24.2)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in d:\\sensemi\\.venv\\lib\\site-packages (from matplotlib->roboflow) (3.2.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in d:\\sensemi\\.venv\\lib\\site-packages (from requests->roboflow) (3.4.0)\n",
      "loading Roboflow workspace...\n",
      "loading Roboflow project...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading Dataset Version Zip in irispupille-1 to yolov5pytorch:: 100%|██████████| 18015/18015 [00:06<00:00, 2704.00it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting Dataset Version Zip to irispupille-1 in yolov5pytorch:: 100%|██████████| 912/912 [00:00<00:00, 3062.30it/s]\n"
     ]
    }
   ],
   "source": [
    "!pip install roboflow\n",
    "\n",
    "from roboflow import Roboflow\n",
    "rf = Roboflow(api_key=\"VW01wCar4Rx8GjuSHfGy\")\n",
    "project = rf.workspace(\"iris-annotation\").project(\"irispupille\")\n",
    "version = project.version(1)\n",
    "dataset = version.download(\"yolov5\")\n",
    "                \n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "D:\\SENSEMI\\yolov5\n"
     ]
    }
   ],
   "source": [
    "%cd \"D:\\\\SENSEMI\\\\yolov5\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "YQ8DYohmPgZg",
    "outputId": "758ed0ea-9867-4a60-f8a3-91a8c56a4182"
   },
   "outputs": [],
   "source": [
    "%%capture output\n",
    "!python train.py --img 640 --batch 16 --epochs 150 --data \"D:\\\\SENSEMI\\\\yolov5\\\\irispupille-1\\\\data.yaml\" --weights yolov5s.pt --cache"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "k4D-P662rlbU"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The path exists.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "path =\"D:\\\\SENSEMI\\\\yolov5\\\\irispupille-1\\\\data.yaml\"\n",
    "\n",
    "if os.path.exists(path):\n",
    "    print(\"The path exists.\")\n",
    "else:\n",
    "    print(\"The path does not exist.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "U54mNUVuAXVY",
    "outputId": "88656e08-b593-4a24-a0d4-aa453469ad64"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mdetect: \u001b[0mweights=['D:\\\\\\\\SENSEMI\\\\\\\\yolov5\\\\\\\\runs\\\\\\\\train\\\\\\\\exp\\\\\\\\weights\\\\\\\\best.pt'], source=D:\\\\SENSEMI\\\\yolov5\\\\enhanced_eyes, data=data\\coco128.yaml, imgsz=[640, 640], conf_thres=0.5, iou_thres=0.45, max_det=1000, device=, view_img=False, save_txt=False, save_format=0, save_csv=False, save_conf=False, save_crop=False, nosave=False, classes=None, agnostic_nms=False, augment=False, visualize=False, update=False, project=runs\\detect, name=exp, exist_ok=False, line_thickness=3, hide_labels=False, hide_conf=False, half=False, dnn=False, vid_stride=1\n",
      "fatal: detected dubious ownership in repository at 'D:/SENSEMI/yolov5'\n",
      "'D:/SENSEMI/yolov5' is owned by:\n",
      "\tBUILTIN/Administrators (S-1-5-32-544)\n",
      "but the current user is:\n",
      "\tAYUSH/HP (S-1-5-21-2724557807-3718659800-2024191519-1001)\n",
      "To add an exception for this directory, call:\n",
      "\n",
      "\tgit config --global --add safe.directory D:/SENSEMI/yolov5\n",
      "YOLOv5  2024-12-15 Python-3.10.10 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "image 1/14 D:\\SENSEMI\\yolov5\\enhanced_eyes\\aL.jpg: 448x640 1 iris, 1 pupille, 326.4ms\n",
      "image 2/14 D:\\SENSEMI\\yolov5\\enhanced_eyes\\aR.jpg: 544x640 1 iris, 1 pupille, 317.0ms\n",
      "image 3/14 D:\\SENSEMI\\yolov5\\enhanced_eyes\\bL.jpg: 448x640 1 iris, 177.3ms\n",
      "image 4/14 D:\\SENSEMI\\yolov5\\enhanced_eyes\\bR.jpg: 576x640 2 iriss, 282.8ms\n",
      "image 5/14 D:\\SENSEMI\\yolov5\\enhanced_eyes\\himL.jpg: 512x640 1 iris, 1 pupille, 331.1ms\n",
      "image 6/14 D:\\SENSEMI\\yolov5\\enhanced_eyes\\himR.jpg: 576x640 1 iris, 1 pupille, 255.1ms\n",
      "image 7/14 D:\\SENSEMI\\yolov5\\enhanced_eyes\\mL.jpg: 512x640 1 iris, 1 pupille, 212.8ms\n",
      "image 8/14 D:\\SENSEMI\\yolov5\\enhanced_eyes\\mR.jpg: 384x640 1 iris, 1 pupille, 208.7ms\n",
      "image 9/14 D:\\SENSEMI\\yolov5\\enhanced_eyes\\rL.jpg: 544x640 1 iris, 1 pupille, 276.7ms\n",
      "image 10/14 D:\\SENSEMI\\yolov5\\enhanced_eyes\\rR.jpg: 576x640 2 iriss, 208.6ms\n",
      "image 11/14 D:\\SENSEMI\\yolov5\\enhanced_eyes\\sL.jpg: 480x640 2 iriss, 260.8ms\n",
      "image 12/14 D:\\SENSEMI\\yolov5\\enhanced_eyes\\sR.jpg: 544x640 1 iris, 244.8ms\n",
      "image 13/14 D:\\SENSEMI\\yolov5\\enhanced_eyes\\yL.jpg: 512x640 1 iris, 259.0ms\n",
      "image 14/14 D:\\SENSEMI\\yolov5\\enhanced_eyes\\yR.jpg: 480x640 1 iris, 212.8ms\n",
      "Speed: 1.9ms pre-process, 255.3ms inference, 2.0ms NMS per image at shape (1, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\detect\\exp2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python detect.py --weights \"D:\\\\SENSEMI\\\\yolov5\\\\runs\\\\train\\\\exp\\\\weights\\\\best.pt\" --source \"D:\\\\SENSEMI\\\\yolov5\\\\enhanced_eyes\" --conf-thres 0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running custom YOLOv5 test script...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "fatal: detected dubious ownership in repository at 'D:/SENSEMI/yolov5'\n",
      "'D:/SENSEMI/yolov5' is owned by:\n",
      "\tBUILTIN/Administrators (S-1-5-32-544)\n",
      "but the current user is:\n",
      "\tAYUSH/HP (S-1-5-21-2724557807-3718659800-2024191519-1001)\n",
      "To add an exception for this directory, call:\n",
      "\n",
      "\tgit config --global --add safe.directory D:/SENSEMI/yolov5\n",
      "YOLOv5  2024-12-15 Python-3.10.10 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mScanning D:\\SENSEMI\\yolov5\\irispupille-1\\test\\labels...:   0%|          | 0/45 [00:00<?, ?it/s]\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mScanning D:\\SENSEMI\\yolov5\\irispupille-1\\test\\labels... 1 images, 0 backgrounds, 0 corrupt:   2%|▏         | 1/45 [00:17<12:30, 17.05s/it]\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mScanning D:\\SENSEMI\\yolov5\\irispupille-1\\test\\labels... 45 images, 0 backgrounds, 0 corrupt: 100%|██████████| 45/45 [00:17<00:00,  2.63it/s]\n",
      "\u001b[34m\u001b[1mtest: \u001b[0mNew cache created: D:\\SENSEMI\\yolov5\\irispupille-1\\test\\labels.cache\n",
      "\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:   0%|          | 0/2 [00:00<?, ?it/s]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95:  50%|█████     | 1/2 [03:31<03:31, 211.49s/it]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [05:11<00:00, 145.87s/it]\n",
      "                 Class     Images  Instances          P          R      mAP50   mAP50-95: 100%|██████████| 2/2 [05:11<00:00, 155.71s/it]\n",
      "                   all         45         90       0.99       0.99      0.995      0.901\n",
      "                  iris         45         45          1      0.981      0.995      0.929\n",
      "               pupille         45         45      0.979          1      0.995      0.873\n",
      "Speed: 9.6ms pre-process, 6885.4ms inference, 2.3ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns\\val\\test_results\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python test.py --weights \"D:\\\\SENSEMI\\\\yolov5\\\\runs\\\\train\\\\exp\\\\weights\\\\best.pt\" --data \"D:\\\\SENSEMI\\\\yolov5\\\\irispupille-1\\\\data.yaml\" --img 640\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Eyes detected and saved for person 1.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load the eye detector\n",
    "eye_cascade = cv2.CascadeClassifier(cv2.data.haarcascades + 'haarcascade_eye.xml')\n",
    "\n",
    "# Directory to store cropped eye images\n",
    "output_dir = \"detected_eyes\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Start video capture\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# Flags to track detected eyes\n",
    "person_counter = 1  # Counter to store images for different people\n",
    "\n",
    "while True:\n",
    "    ret, img = cap.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)\n",
    "    eyes = eye_cascade.detectMultiScale(gray, 1.1, 4)\n",
    "\n",
    "    for (ex, ey, ew, eh) in eyes:\n",
    "        eye_center_x = ex + ew // 2\n",
    "        frame_center_x = img.shape[1] // 2\n",
    "\n",
    "        # Determine if it's the left or right eye based on position\n",
    "        if eye_center_x < frame_center_x:\n",
    "            eye_label = \"Left Eye\"\n",
    "            color = (0, 255, 0)  # Green for left eye\n",
    "        else:\n",
    "            eye_label = \"Right Eye\"\n",
    "            color = (255, 0, 0)  # Blue for right eye\n",
    "\n",
    "        # Draw a rectangle around the detected eye\n",
    "        cv2.rectangle(img, (ex, ey), (ex + ew, ey + eh), color, 2)\n",
    "        cv2.putText(img, eye_label, (ex, ey - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.5, color, 2)\n",
    "\n",
    "    # Display the frame with detected eyes\n",
    "    cv2.imshow('Eye Detection', img)\n",
    "\n",
    "    key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "    if key == ord('s'):  # Save the eyes when 's' is pressed\n",
    "        for (ex, ey, ew, eh) in eyes:\n",
    "            eye_center_x = ex + ew // 2\n",
    "            frame_center_x = img.shape[1] // 2\n",
    "\n",
    "            if eye_center_x < frame_center_x:\n",
    "                eye_label = \"Left Eye\"\n",
    "                eye_region = img[ey:ey + eh, ex:ex + ew]\n",
    "                cv2.imwrite(os.path.join(output_dir, f\"person_{person_counter}_left_eye.jpg\"), eye_region)\n",
    "            else:\n",
    "                eye_label = \"Right Eye\"\n",
    "                eye_region = img[ey:ey + eh, ex:ex + ew]\n",
    "                cv2.imwrite(os.path.join(output_dir, f\"person_{person_counter}_right_eye.jpg\"), eye_region)\n",
    "\n",
    "        print(f\"Eyes detected and saved for person {person_counter}.\")\n",
    "        person_counter += 1\n",
    "\n",
    "    elif key == ord('q'):  # Quit when 'q' is pressed\n",
    "        break\n",
    "\n",
    "# Release the capture and close windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enhanced image saved to enhanced_eyes\\aL.jpg\n",
      "Enhanced image saved to enhanced_eyes\\aR.jpg\n",
      "Enhanced image saved to enhanced_eyes\\bL.jpg\n",
      "Enhanced image saved to enhanced_eyes\\bR.jpg\n",
      "Enhanced image saved to enhanced_eyes\\himL.jpg\n",
      "Enhanced image saved to enhanced_eyes\\himR.jpg\n",
      "Enhanced image saved to enhanced_eyes\\mL.jpg\n",
      "Enhanced image saved to enhanced_eyes\\mR.jpg\n",
      "Enhanced image saved to enhanced_eyes\\rL.jpg\n",
      "Enhanced image saved to enhanced_eyes\\rR.jpg\n",
      "Enhanced image saved to enhanced_eyes\\sL.jpg\n",
      "Enhanced image saved to enhanced_eyes\\sR.jpg\n",
      "Enhanced image saved to enhanced_eyes\\yL.jpg\n",
      "Enhanced image saved to enhanced_eyes\\yR.jpg\n",
      "All images enhanced and saved in the 'enhanced_eyes' directory.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# Directory to store cropped eye images\n",
    "input_dir = \"C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\table\"\n",
    "output_dir = \"enhanced_eyes\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "def remove_specular_reflection(image):\n",
    "    \"\"\"\n",
    "    Removes specular reflections from the eye image using inpainting.\n",
    "    \"\"\"\n",
    "    # Threshold to create a mask for bright spots (specular reflections)\n",
    "    _, mask = cv2.threshold(image, 230, 255, cv2.THRESH_BINARY)  # Adjust threshold as needed\n",
    "    \n",
    "    # Dilate the mask to cover surrounding regions of reflections\n",
    "    kernel = cv2.getStructuringElement(cv2.MORPH_ELLIPSE, (3, 3))\n",
    "    mask = cv2.dilate(mask, kernel, iterations=1)\n",
    "    \n",
    "    # Inpaint the regions covered by the mask\n",
    "    inpainted_image = cv2.inpaint(image, mask, inpaintRadius=5, flags=cv2.INPAINT_TELEA)\n",
    "    return inpainted_image\n",
    "\n",
    "def enhance_image(image_path, output_path):\n",
    "    # Load the image in grayscale\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "\n",
    "    # Step 1: Remove Specular Reflections\n",
    "    no_reflection_image = remove_specular_reflection(image)\n",
    "\n",
    "    # Step 2: Apply CLAHE (Contrast Limited Adaptive Histogram Equalization)\n",
    "    clahe = cv2.createCLAHE(clipLimit=3.0, tileGridSize=(8, 8))\n",
    "    clahe_image = clahe.apply(no_reflection_image)\n",
    "\n",
    "    # Step 3: Apply Bilateral Filtering for noise reduction while preserving edges\n",
    "    bilateral_filtered_image = cv2.bilateralFilter(clahe_image, d=15, sigmaColor=100, sigmaSpace=100)\n",
    "\n",
    "    # Step 4: Apply Sharpening\n",
    "    sharpening_kernel = np.array([[0, -1, 0], [-1, 5.5, -1], [0, -1, 0]])\n",
    "    sharpened_image = cv2.filter2D(bilateral_filtered_image, -1, sharpening_kernel)\n",
    "\n",
    "    # Save the enhanced image\n",
    "    cv2.imwrite(output_path, sharpened_image)\n",
    "\n",
    "# Process each image in the detected_eyes directory\n",
    "for filename in os.listdir(input_dir):\n",
    "    input_path = os.path.join(input_dir, filename)\n",
    "\n",
    "    # Skip if it's not an image file\n",
    "    if not (filename.lower().endswith('.jpg') or filename.lower().endswith('.png')):\n",
    "        continue\n",
    "\n",
    "    # Generate output file path\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "\n",
    "    # Enhance the image\n",
    "    enhance_image(input_path, output_path)\n",
    "    print(f\"Enhanced image saved to {output_path}\")\n",
    "\n",
    "print(\"All images enhanced and saved in the 'enhanced_eyes' directory.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load your trained YOLOv5 model\n",
    "model_path = \"D:\\\\SENSEMI\\\\yolov5\\\\runs\\\\train\\\\exp\\\\weights\\\\best.pt\"\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path)\n",
    "\n",
    "# Paths for enhanced left and right eye images\n",
    "enhanced_images_dir = \"D:\\\\SENSEMI\\\\enhanced_eyes\"\n",
    "output_dir = \"D:\\\\SENSEMI\\\\processed_eyes\"\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Class indices for iris and pupil (update according to your dataset)\n",
    "IRIS_CLASS = 0  # Example: Class ID for iris\n",
    "PUPIL_CLASS = 1  # Example: Class ID for pupil\n",
    "\n",
    "# Function to process an image and calculate the ratio\n",
    "def process_eye(image_path, eye_label):\n",
    "    results = model.predict(source=image_path, conf=0.5, save=False)\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    iris_width = None\n",
    "    pupil_width = None\n",
    "\n",
    "    # Process detection results\n",
    "    for result in results:\n",
    "        for box, cls in zip(result.boxes.xyxy, result.boxes.cls):\n",
    "            x_min, y_min, x_max, y_max = map(int, box)\n",
    "            width = x_max - x_min\n",
    "\n",
    "            # Check class and assign width\n",
    "            if cls == IRIS_CLASS:\n",
    "                iris_width = width\n",
    "                color = (255, 0, 0)  # Blue for iris\n",
    "            elif cls == PUPIL_CLASS:\n",
    "                pupil_width = width\n",
    "                color = (0, 255, 0)  # Green for pupil\n",
    "\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "\n",
    "    # Compute and display ratio in console\n",
    "    ratio_text = f\"{eye_label} Ratio: Not Detected\"\n",
    "    if iris_width and pupil_width:\n",
    "        ratio = iris_width / pupil_width  # Changed to iris-to-pupil ratio\n",
    "        ratio_text = f\"{eye_label} IP Ratio: {ratio:.2f}\"\n",
    "        print(f\"{os.path.basename(image_path)}: {ratio_text}\")\n",
    "    else:\n",
    "        print(f\"{os.path.basename(image_path)}: {ratio_text}\")\n",
    "\n",
    "    # Save the processed image with only bounding boxes\n",
    "    output_path = os.path.join(output_dir, os.path.basename(image_path))\n",
    "    cv2.imwrite(output_path, img)\n",
    "    print(f\"Processed image saved to: {output_path}\")\n",
    "\n",
    "# Process all enhanced images for left and right eyes\n",
    "for filename in os.listdir(enhanced_images_dir):\n",
    "    filepath = os.path.join(enhanced_images_dir, filename)\n",
    "    if \"left_eye\" in filename.lower():\n",
    "        process_eye(filepath, \"Left Eye\")\n",
    "    elif \"right_eye\" in filename.lower():\n",
    "        process_eye(filepath, \"Right Eye\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\SENSEMI\\.venv\\lib\\site-packages\\torch\\hub.py:330: UserWarning: You are about to download and run code from an untrusted repository. In a future release, this won't be allowed. To add the repository to your trusted list, change the command to {calling_fn}(..., trust_repo=False) and a command prompt will appear asking for an explicit confirmation of trust, or load(..., trust_repo=True), which will assume that the prompt is to be answered with 'yes'. You can also use load(..., trust_repo='check') which will only prompt for confirmation if the repo is not already trusted. This will eventually be the default behaviour\n",
      "  warnings.warn(\n",
      "Downloading: \"https://github.com/ultralytics/yolov5/zipball/master\" to C:\\Users\\HP/.cache\\torch\\hub\\master.zip\n",
      "YOLOv5  2024-12-17 Python-3.10.10 torch-2.5.1+cpu CPU\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7015519 parameters, 0 gradients, 15.8 GFLOPs\n",
      "Adding AutoShape... \n",
      "C:\\Users\\HP/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aL.jpg: IP Ratio = 2.84\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aR.jpg: IP Ratio = 0.46\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bL.jpg: IP Ratio = 2.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bR.jpg: IP Ratio = 1.38\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "himL.jpg: IP Ratio = 1.07\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "himR.jpg: IP Ratio = 0.93\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mL.jpg: IP Ratio = 1.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mR.jpg: IP Ratio = 1.03\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rL.jpg: IP Ratio = 2.37\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "rR.jpg: IP Ratio = Not Detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sL.jpg: IP Ratio = 0.32\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sR.jpg: IP Ratio = Not Detected\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yL.jpg: IP Ratio = 3.70\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\HP/.cache\\torch\\hub\\ultralytics_yolov5_master\\models\\common.py:892: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "yR.jpg: IP Ratio = Not Detected\n"
     ]
    }
   ],
   "source": [
    "#custom names\n",
    "import torch\n",
    "import cv2\n",
    "import os\n",
    "\n",
    "# Load the YOLOv5 model\n",
    "model_path = \"D:\\\\SENSEMI\\\\yolov5\\\\runs\\\\train\\\\exp\\\\weights\\\\best.pt\"\n",
    "model = torch.hub.load('ultralytics/yolov5', 'custom', path=model_path)\n",
    "\n",
    "# Input and output directories\n",
    "enhanced_images_dir = \"D:\\\\SENSEMI\\\\yolov5\\\\enhanced_eyes\"  # Directory containing generic images\n",
    "output_dir = \"D:\\\\SENSEMI\\\\processed_eyes\"  # Output directory for processed images\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "# Class indices for iris and pupil\n",
    "IRIS_CLASS = 0  # Class ID for iris\n",
    "PUPIL_CLASS = 1  # Class ID for pupil\n",
    "\n",
    "# Function to process an image and calculate the IP ratio\n",
    "def process_image(image_path):\n",
    "    results = model(image_path)  # YOLOv5 model inference\n",
    "    img = cv2.imread(image_path)\n",
    "\n",
    "    iris_width = None\n",
    "    pupil_width = None\n",
    "\n",
    "    # Process detection results\n",
    "    for *box, conf, cls in results.xyxy[0]:  # Extract bounding boxes, confidence, and class\n",
    "        x_min, y_min, x_max, y_max = map(int, box)\n",
    "        width = x_max - x_min\n",
    "\n",
    "        # Assign width based on class\n",
    "        if cls == IRIS_CLASS:\n",
    "            iris_width = width\n",
    "            color = (255, 0, 0)  # Blue for iris\n",
    "        elif cls == PUPIL_CLASS:\n",
    "            pupil_width = width\n",
    "            color = (0, 255, 0)  # Green for pupil\n",
    "\n",
    "        # Draw bounding box on the image\n",
    "        cv2.rectangle(img, (x_min, y_min), (x_max, y_max), color, 2)\n",
    "\n",
    "    # Compute and log the IP ratio\n",
    "    filename = os.path.basename(image_path)\n",
    "    if iris_width and pupil_width:\n",
    "        ratio = iris_width / pupil_width  # Iris-to-pupil ratio\n",
    "        print(f\"{filename}: IP Ratio = {ratio:.2f}\")\n",
    "    else:\n",
    "        print(f\"{filename}: IP Ratio = Not Detected\")\n",
    "\n",
    "    # Save the processed image with bounding boxes\n",
    "    output_path = os.path.join(output_dir, filename)\n",
    "    cv2.imwrite(output_path, img)\n",
    "\n",
    "# Process all valid images in the directory\n",
    "for filename in os.listdir(enhanced_images_dir):\n",
    "    filepath = os.path.join(enhanced_images_dir, filename)\n",
    "\n",
    "    # Ensure the file is a valid image format\n",
    "    if filename.lower().endswith((\".jpg\", \".jpeg\", \".png\", \".bmp\")):\n",
    "        process_image(filepath)\n",
    "    else:\n",
    "        print(f\"Skipped non-image file: {filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO\n",
    "model=YOLO(\"yolo11n.pt\")\n",
    "trained_results=model.train(data=\"D:\\\\mp\\\\data.yaml\", epochs=100,imgsz=416,device=\"cpu\")\n",
    "metrices=model.val()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ultralytics 8.3.31 🚀 Python-3.10.10 torch-2.5.1+cpu CPU (13th Gen Intel Core(TM) i5-1335U)\n",
      "YOLO11n summary (fused): 238 layers, 2,582,542 parameters, 0 gradients, 6.3 GFLOPs\n",
      "\n",
      "image 1/1 C:\\Users\\HP\\OneDrive\\Desktop\\pdd_L.jpg: 320x416 1 iris, 3 pupils, 161.7ms\n",
      "Speed: 4.0ms preprocess, 161.7ms inference, 2.0ms postprocess per image at shape (1, 3, 320, 416)\n",
      "Results saved to \u001b[1mruns\\detect\\predict5\u001b[0m\n",
      "💡 Learn more at https://docs.ultralytics.com/modes/predict\n"
     ]
    }
   ],
   "source": [
    "!yolo predict model=\"D:\\\\mp\\\\runs\\\\detect\\\\train\\\\weights\\\\best.pt\" source=\"C:\\\\Users\\\\HP\\\\OneDrive\\\\Desktop\\\\pdd_L.jpg\" conf=0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
